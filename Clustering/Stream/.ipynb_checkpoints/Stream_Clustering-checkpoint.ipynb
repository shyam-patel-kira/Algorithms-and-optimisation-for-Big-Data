{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Stream Clustering\n",
    "### Dataset: 300 Features and 400 Data Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import seaborn as sns\n",
    "from scipy.spatial import distance\n",
    "from sklearn.datasets import (make_blobs,make_circles,make_moons)\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:  (400, 300)\n",
      "Validation Data:  (60, 300)\n"
     ]
    }
   ],
   "source": [
    "# To get demographics of data\n",
    "train_data = pd.read_excel('Shared for Clustering.xlsx', \"Training\", header=None)\n",
    "validation_data = pd.read_excel('Shared for Clustering.xlsx', \"Validation\", header=None)\n",
    "print(\"Train Data: \", train_data.shape)\n",
    "print(\"Validation Data: \", validation_data.shape)\n",
    "train_data.to_csv(\"Train.csv\", index=None, header=False)\n",
    "validation_data.to_csv(\"Validation.csv\", index=None, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream Generator function\n",
    "# Total 40 chunks, each of size 10\n",
    "\n",
    "def generateStream(chunk_size = 10):\n",
    "    count = 0\n",
    "    for chunk in pd.read_csv('Train.csv', header=None, chunksize=chunk_size):\n",
    "        chunk_array = chunk.values\n",
    "        yield chunk_array\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define My Clustering Function\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from numpy.random import choice\n",
    "from numpy.random import seed\n",
    "from pyclustering.cluster.kmeans import kmeans\n",
    "\n",
    "def KMeans(data, k=2):\n",
    "    \n",
    "    seed(1)\n",
    "    random_rows = choice(len(data), size=k, replace=False)\n",
    "    random_centroids = data[random_rows, :]\n",
    "    \n",
    "    k_means = kmeans(data, random_centroids)\n",
    "    k_means.process()\n",
    "    clusters = k_means.get_clusters()\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateCentroids(data, cluster, k):\n",
    "    \n",
    "    centroids = []\n",
    "    for i in range(0, k):\n",
    "        m = np.mean(data[cluster[i]], axis = 0)\n",
    "        centroids.append(m)\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering on stream data\n",
    "\n",
    "## Getting stream data\n",
    "sdata = generateStream(10)  # <- Yeilds 40 chunks each of size 10\n",
    "\n",
    "# initializing other parameters\n",
    "k = 2\n",
    "level1=[]\n",
    "count1 = 0\n",
    "level2 = []\n",
    "count2 = 0\n",
    "level3 = []\n",
    "count3 = 0\n",
    "cluster_weightage = [0, 0]\n",
    "\n",
    "for data in sdata:\n",
    "    cluster = KMeans(data, k)\n",
    "    centroids = calculateCentroids(data, cluster, k)\n",
    "    for tmp in range(0, k):\n",
    "        level1.append(centroids[tmp])\n",
    "    count1 = count1 + 1\n",
    "    \n",
    "    if count1==5:\n",
    "        data2 = np.array(level1)\n",
    "        #print(data1)\n",
    "        cluster2 = KMeans(data2, k)\n",
    "        centroids2 = calculateCentroids(data2, cluster2, k)\n",
    "        for tmp2 in range(0, k):\n",
    "            level2.append(centroids2[tmp2])\n",
    "        count1 = 0\n",
    "        count2 = count2 + 1\n",
    "        level1 = []\n",
    "        \n",
    "        if count2==4:\n",
    "            data3 = np.array(level2)\n",
    "            cluster3 = KMeans(data3, k)\n",
    "            centroids3 = calculateCentroids(data3, cluster3, k)\n",
    "            for tmp3 in range(0, k):\n",
    "                level3.append(centroids3[tmp3])\n",
    "            count2 = 0\n",
    "            count3 = count3 + 1\n",
    "            level2 = []\n",
    "            \n",
    "\n",
    "# Generating final cluster\n",
    "data4 = np.array(level3)\n",
    "cluster4 = KMeans(data4, k)\n",
    "centroids4 = calculateCentroids(data4, cluster4, k)\n",
    "final_centroids = []\n",
    "for tmp4 in range(0, k):\n",
    "    final_centroids.append(centroids4[tmp4])\n",
    "        \n",
    "# print(\"Final Centroids: \")\n",
    "# pprint(final_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "def calculateDistance(final_centroids, data_row):\n",
    "    \n",
    "    op = -1\n",
    "    d = 0\n",
    "    min_dist = sys.float_info.max\n",
    "    for row in range(0, k):\n",
    "        d = distance.euclidean(data_row, final_centroids[row])\n",
    "        if d < min_dist:\n",
    "            min_dist = d\n",
    "            op = row\n",
    "            \n",
    "    return op+1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "\n",
    "def validate(final_centroids):\n",
    "    \n",
    "    validation_data = pd.read_csv(\"Validation.csv\", header=None)\n",
    "    print(validation_data.shape)\n",
    "    \n",
    "    output_class = []\n",
    "    data_row = validation_data.values\n",
    "    for val in range(0, validation_data.shape[0]):\n",
    "        output_class.append(calculateDistance(final_centroids, data_row[val, :]))\n",
    "        \n",
    "    print(\"Output Class for Validation Data:\")\n",
    "    print(output_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 300)\n",
      "Output Class for Validation Data:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Validate\n",
    "validate(final_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[1, 2],\n",
      "       [3, 4],\n",
      "       [5, 6]])\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Create Predict Function\n",
    "\n",
    "def predict(final_centroids):\n",
    "    \n",
    "    # Take \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "a = [[2], [3, 4]]\n",
    "print(len(a[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
